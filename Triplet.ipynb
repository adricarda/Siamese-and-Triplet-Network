{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Triplet.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z0RWPcL4VY2",
        "colab_type": "text"
      },
      "source": [
        "# Triplet networks & one-shot learning\n",
        "\n",
        "Train a triplet network on the first 80 classes of (the training set of) Cifar-100\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_KTeRjS4qb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization, merge, concatenate, Activation, Concatenate\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.datasets import cifar100\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import TruncatedNormal, RandomNormal\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from itertools import permutations\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6z2aNwH4Kdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "input_size = (32, 32, 3)\n",
        "# embedding_dimensions = 128\n",
        "batch_size = 256\n",
        "num_classes = 100\n",
        "num_train_classes = 80\n",
        "num_test_classes = num_classes - num_train_classes\n",
        "\n",
        "(cifar_x_train, cifar_y_train), (cifar_x_test, cifar_y_test) = cifar100.load_data(label_mode='fine')\n",
        "\n",
        "# reshape output labels\n",
        "cifar_y_train = cifar_y_train.reshape(-1)\n",
        "cifar_y_test = cifar_y_test.reshape(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypRnlwEq4nGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "9159fe2a-2d23-4343-b75f-564ade9e6c9f"
      },
      "source": [
        "print(cifar_x_train.shape)\n",
        "print(cifar_y_train.shape)\n",
        "print(cifar_x_test.shape)\n",
        "print(cifar_y_test.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000,)\n",
            "(10000, 32, 32, 3)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JtME6Li4zNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted_indices = np.argsort(cifar_y_train)\n",
        "cifar_x_train = cifar_x_train[sorted_indices]\n",
        "cifar_y_train = cifar_y_train[sorted_indices]\n",
        "split_index = np.searchsorted(cifar_y_train, num_train_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdFTia2x41Dk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d7e3d0c7-8a61-48e4-d54c-42832c692e9e"
      },
      "source": [
        "x_train = cifar_x_train[:split_index]\n",
        "y_train = cifar_y_train[:split_index]\n",
        "\n",
        "# take the last 20 classes as test\n",
        "x_validation = cifar_x_train[split_index:]\n",
        "y_validation = cifar_y_train[split_index:]\n",
        "\n",
        "# NOTE: assumption that the number of samples\n",
        "#       are divisible by the number of classes\n",
        "x_train = x_train.reshape(num_train_classes, x_train.shape[0]//num_train_classes, *x_train.shape[1:])\n",
        "x_validation = x_validation.reshape(num_test_classes, x_validation.shape[0]//num_test_classes, *x_validation.shape[1:])\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80, 500, 32, 32, 3)\n",
            "(40000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyo5OYt745H_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(batch_size, X):\n",
        "    \"\"\"Create batch of n triplets anchor, positive, negative\"\"\"\n",
        "    n_classes, n_examples, w, h, d = X.shape\n",
        "    # randomly sample several classes to use in the batch\n",
        "    categories = np.random.choice(n_classes, size=(batch_size,), replace=False)\n",
        "    # initialize 3 empty arrays for the input image batch\n",
        "    triplet = [np.zeros((batch_size, h, w, 3)) for i in range(3)]\n",
        "    # initialize dummy vector fot labels (useless in triplenet)\n",
        "    targets = np.zeros((batch_size,))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      category = categories[i]\n",
        "      idx_1, idx_2 = random.sample(range(n_examples),k=2)\n",
        "      triplet[0][i, :, :, :] = X[category, idx_1].reshape(w, h, 3)\n",
        "      triplet[1][i, :, :, :] = X[category, idx_2].reshape(w, h, 3)\n",
        "      idx_3 = np.random.randint(0, n_examples)\n",
        "      category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
        "      triplet[2][i, :, :, :] = X[category_2,idx_3].reshape(w, h, 3)\n",
        "    return triplet, targets\n",
        "\n",
        "def batch_generator(batch_size, X):\n",
        "    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
        "    while True:\n",
        "      triplet, targets = get_batch(batch_size, X)\n",
        "      yield (triplet, targets)\n",
        "\n",
        "def train(model, X_train, batch_size=64, steps_per_epoch=250, epochs=1):\n",
        "    model.fit_generator(batch_generator(batch_size, X_train), \n",
        "                        steps_per_epoch=steps_per_epoch, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l0qJNXR46gZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "202f6b46-7c53-4260-b64e-dac5ef1341b0"
      },
      "source": [
        "#test: the first 2 images should be in the same class\n",
        "for b in batch_generator(2,x_train):\n",
        "  a = np.array(b[0], np.uint8)\n",
        "  print(a.shape)\n",
        "  image1 = a[0][0]\n",
        "  image2 = a[1][0]\n",
        "  image3 = a[2][0]\n",
        "  image4 = a[0][1]\n",
        "  image5 = a[1][1]\n",
        "  image6 = a[2][1]\n",
        "  fig=plt.figure(figsize=(4, 4))\n",
        "  for i in range(1,7):\n",
        "    fig.add_subplot(2, 3, i)\n",
        "    if i==1:\n",
        "      plt.imshow(image1)\n",
        "    elif i==2:\n",
        "      plt.imshow(image2)\n",
        "    elif i==3:\n",
        "      plt.imshow(image3)\n",
        "    elif i==4:\n",
        "      plt.imshow(image4)\n",
        "    elif i==5:\n",
        "      plt.imshow(image5)\n",
        "    elif i==6:\n",
        "      plt.imshow(image6)      \n",
        "  plt.show()\n",
        "  break"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 2, 32, 32, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAADbCAYAAACLF+9EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F9JIjE_48pH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "def triplet_loss(ground_truth, network_output):\n",
        "    anchor, positive, negative = tf.split(network_output, num_or_size_splits=3, axis=1)\n",
        "    # distance between the anchor and the positive\n",
        "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
        "\n",
        "    # distance between the anchor and the negative\n",
        "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
        "\n",
        "    # compute loss\n",
        "    basic_loss = K.mean(pos_dist-neg_dist+0.5)\n",
        "    loss = K.maximum(basic_loss,0.0)\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pEAE42h5Pvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = (32, 32, 3)\n",
        "anchor_input = Input(input_shape, name='anchor')\n",
        "positive_input = Input(input_shape, name='positive')\n",
        "negative_input = Input(input_shape, name='negative')\n",
        "\n",
        "# build convnet to use in each siamese 'leg'\n",
        "conv_input = Input(input_shape)\n",
        "tripleNet = Sequential()\n",
        "\n",
        "tripleNet.add(Conv2D(64, (3,3), input_shape=input_shape, padding='SAME'))\n",
        "tripleNet.add(BatchNormalization())\n",
        "tripleNet.add(LeakyReLU(alpha=0.1))\n",
        "tripleNet.add(MaxPooling2D())\n",
        "tripleNet.add(Dropout(0.3))\n",
        "\n",
        "tripleNet.add(Conv2D(128, (3,3), input_shape=input_shape, padding='SAME'))\n",
        "tripleNet.add(BatchNormalization())\n",
        "tripleNet.add(LeakyReLU(alpha=0.1))\n",
        "tripleNet.add(MaxPooling2D())\n",
        "tripleNet.add(Dropout(0.3))\n",
        "\n",
        "tripleNet.add(Conv2D(256, (3,3), input_shape=input_shape, padding='SAME'))\n",
        "tripleNet.add(BatchNormalization())\n",
        "tripleNet.add(LeakyReLU(alpha=0.1))\n",
        "tripleNet.add(MaxPooling2D())\n",
        "tripleNet.add(Dropout(0.3))\n",
        "\n",
        "tripleNet.add(Flatten())\n",
        "\n",
        "tripleNet.add(Dense(1024, kernel_regularizer=l2(1e-3)))\n",
        "tripleNet.add(Dense(128, kernel_regularizer=l2(1e-3)))\n",
        "tripleNet.add(Lambda(lambda t: K.l2_normalize(t, axis=1)))\n",
        "\n",
        "anchor_tp_leg = tripleNet(anchor_input)\n",
        "positive_tp_leg = tripleNet(positive_input)\n",
        "negative_tp_leg = tripleNet(negative_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cmk_mRN5Re7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "65456b93-1ceb-4820-c45c-bd3094d238f8"
      },
      "source": [
        "concat = Concatenate(axis=-1, name='concatenate')([anchor_tp_leg,\n",
        "                                                   positive_tp_leg,\n",
        "                                                   negative_tp_leg])\n",
        "\n",
        "triplet_net = Model(inputs=[anchor_input, positive_input,\n",
        "                            negative_input], outputs=concat)\n",
        "triplet_net.compile(loss=triplet_loss,\n",
        "                    optimizer=Adam(lr=0.0001, amsgrad=True, decay=0.0000001))\n",
        "\n",
        "triplet_net.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "anchor (InputLayer)             (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positive (InputLayer)           (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "negative (InputLayer)           (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       (None, 128)          4699136     anchor[0][0]                     \n",
            "                                                                 positive[0][0]                   \n",
            "                                                                 negative[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 384)          0           sequential_2[1][0]               \n",
            "                                                                 sequential_2[2][0]               \n",
            "                                                                 sequential_2[3][0]               \n",
            "==================================================================================================\n",
            "Total params: 4,699,136\n",
            "Trainable params: 4,698,240\n",
            "Non-trainable params: 896\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX9vh7iS5Wqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def triplet_make_oneshot_task(N, X, start_id=80):\n",
        "  \"\"\"\n",
        "    Generate a test image and its corresponding support set.\n",
        "    It returns the image, its ground truth label, the support set\n",
        "    and the associated labels for each support set image.\n",
        "    \n",
        "    start_class_id parameter is necessary in case the classes from X set\n",
        "    do not start from id 0. (Eg. Cifar100 tested on last 20 classes).\n",
        "  \"\"\"\n",
        "  n_classes, n_examples, w, h, chn = X.shape\n",
        "  categories = np.random.choice(n_classes, N, replace=False)\n",
        "  # take N+1 so that two indices can be used to draw from same category\n",
        "  samples_ids = np.random.choice(n_examples, N+1, replace=False)\n",
        "\n",
        "  true_category = categories[0]+start_id\n",
        "  \n",
        "  # the reshape is necessary to later stack this img with the support set\n",
        "  test_image = X[categories[0], samples_ids[0]].reshape(1, w, h, chn)\n",
        "  support_set = np.asarray([X[categories[i], samples_ids[i+1]] for i in range(N)])\n",
        "  \n",
        "  support_categories = np.asarray([categories[i]+start_id for i in range(N)])\n",
        "  \n",
        "  # shuffle the support set\n",
        "  random_indices = np.random.choice(len(support_set),\n",
        "                                    len(support_set), replace=False)\n",
        "  support_set = support_set[random_indices]\n",
        "  support_categories = support_categories[random_indices]\n",
        "\n",
        "  return test_image, true_category, support_set, support_categories\n",
        "\n",
        "# L2-distance (MSE - Mean Square Error)\n",
        "def mean_square_error(a, b):\n",
        "  return ((a - b)**2).mean(axis=None)\n",
        "\n",
        "def triplet_test_oneshot(model, X, N=20, K=250, verbose=True):\n",
        "  \"\"\"\n",
        "    Test average N-way oneshot learning accuracy\n",
        "    using the neural codes obtained by a pretrained triplet net\n",
        "    and L2 distance over k one-shot tasks.\n",
        "  \"\"\"\n",
        "  n_correct = 0\n",
        "  if verbose:\n",
        "      print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(K, N))\n",
        "      \n",
        "  for i in range(K):\n",
        "      test_img, true_label, support_set, support_labels = triplet_make_oneshot_task(N, X)\n",
        "      \n",
        "      test_img_nn = model.predict(test_img)\n",
        "      \n",
        "      ss_neural_codes = model.predict(support_set, batch_size=support_set.shape[0])\n",
        "      \n",
        "      errors = [mean_square_error(test_img_nn, ss_nn)\n",
        "                for ss_nn in ss_neural_codes]\n",
        "      \n",
        "      # check whether the closes image is actually\n",
        "      # of the same class of the test image\n",
        "      # NOTE -> argmin is the index, not the correct class!!!!!!!!!!!!\n",
        "      if true_label == support_labels[np.argmin(errors)]:\n",
        "          n_correct += 1\n",
        "          \n",
        "  # compute the accuracy\n",
        "  percent_correct = (100.0 * n_correct / K)\n",
        "  \n",
        "  if verbose:\n",
        "      print(\"Got an average of {}% accuracy for {}-way one-shot learning\".format(percent_correct, N))\n",
        "      \n",
        "  return percent_correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPq8ywIC5pNc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13ade0dd-1707-4767-a9a8-20b0a69ddd5f"
      },
      "source": [
        "loops = 100\n",
        "best_acc = 0\n",
        "for i in range(loops):\n",
        "    print(\"=== Training loop {} ===\".format(i+1))\n",
        "    train(triplet_net, x_train)\n",
        "    triplet_neural_codes = Model(inputs=anchor_input, outputs=anchor_tp_leg)\n",
        "\n",
        "    test_acc = triplet_test_oneshot(triplet_neural_codes, x_validation, K=250)\n",
        "    if test_acc >= best_acc:\n",
        "        print(\"********* New best one-shot accuracy, saving model ********\")\n",
        "        triplet_neural_codes.save(os.path.join(\".\", \"triplet_neural_codes.h5\"))\n",
        "        best_acc = test_acc"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Training loop 1 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 1.4828\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 7.6% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 2 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 9s 38ms/step - loss: 1.1735\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 10.0% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 3 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.9017\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 4 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.6745\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 5 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.5160\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 6 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.3909\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 7 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.3089\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 8 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.2455\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 9 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.1952\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 10.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 10 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.1564\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 20.8% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 11 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.1382\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 12 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.1061\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 13 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0902\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 14 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0764\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 15 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0737\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 16 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0623\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 17 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0573\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 18 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0501\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 19 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0543\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 20 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0460\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 21 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0390\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 22 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0315\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 23 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0304\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 24 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0292\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 25 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0351\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 26 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0352\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 10.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 27 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0296\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 28 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0252\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 29 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0278\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 11.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 30 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0223\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 31 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0290\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 32 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0237\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 17.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 33 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0212\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 34 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0191\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 35 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0191\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 36 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0245\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 37 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0279\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 38 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0204\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 39 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0170\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 17.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 40 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0181\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 41 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0165\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 17.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 42 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0121\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 43 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0188\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 21.2% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 44 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0161\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 45 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0183\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 46 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0160\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 47 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0163\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 48 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0127\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 49 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0164\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 50 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0126\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 10.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 51 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0169\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 52 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0146\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 10.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 53 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0174\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 54 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0146\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 17.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 55 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0147\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 56 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0124\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 57 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0133\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 58 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0110\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 59 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 9s 38ms/step - loss: 0.0110\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 60 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0114\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 17.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 61 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0130\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 62 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0139\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 63 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0099\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 64 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0127\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 65 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0150\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 66 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 9s 38ms/step - loss: 0.0125\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 67 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0096\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 68 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0109\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 69 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 0.0108\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 70 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0098\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 71 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0096\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 72 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0134\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 73 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0113\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 74 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0078\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 75 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0111\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 76 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0112\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 77 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 9s 38ms/step - loss: 0.0098\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 20.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 78 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0084\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 79 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0089\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 80 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0124\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 81 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0135\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 82 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0099\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 83 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0107\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 84 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0089\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 85 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0120\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 18.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 86 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0084\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 18.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 87 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0102\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 88 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0099\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 89 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0076\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 90 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0077\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 18.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 91 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0091\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 92 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0106\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 93 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0100\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 18.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 94 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0106\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 95 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 39ms/step - loss: 0.0102\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 96 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0115\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 97 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0079\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 98 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0077\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 99 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0073\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 17.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 100 ===\n",
            "Epoch 1/1\n",
            "250/250 [==============================] - 10s 38ms/step - loss: 0.0095\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 16.0% accuracy for 20-way one-shot learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnZ6s0rX6OYF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b74bbf03-3dd0-447d-ce72-498a88cba3d5"
      },
      "source": [
        "print(best_acc)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8liHp3K6bcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}